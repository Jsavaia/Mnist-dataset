# -*- coding: utf-8 -*-
"""Mnist_Mid_module_assignment.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/19Dngbm_zdQSBVFmOPikcMXjbLLa0WgFf
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten, Conv2D, MaxPooling2D
from tensorflow.keras.optimizers import SGD
from tensorflow.keras.losses import SparseCategoricalCrossentropy
from sklearn.model_selection import train_test_split

tf.random.set_seed(42)

from google.colab import drive
drive.mount('/content/drive')

"""**Data exploration and visualization**"""

train_file = '/content/drive/MyDrive/ML projects/archive/fashion-mnist_train.csv'
test_file = '/content/drive/MyDrive/ML projects/archive/fashion-mnist_test.csv'
train_df = pd.read_csv(train_file)
test_df = pd.read_csv(test_file)

display(train_df.head())
display(test_df.head())

print(f"Training Data Shape: {train_df.shape}")
print(f"Test Data Shape: {test_df.shape}")
print("Missing values in Training Data:", train_df.isnull().sum().sum())
print("Missing values in Test Data:", test_df.isnull().sum().sum())

# Describe training dataset
print("Training Data Summary:")
display(train_df.describe())

# Describe test dataset
print("\nTest Data Summary:")
display(test_df.describe())

class_labels = [
    "T-shirt/top", "Trouser", "Pullover", "Dress", "Coat",
    "Sandal", "Shirt", "Sneaker", "Bag", "Ankle boot"
]
# Extract pixel values and reshape for visualization
x_train = train_df.iloc[:, 1:].values
y_train = train_df.iloc[:, 0].values

x_train_reshaped = x_train.reshape(-1, 28, 28)  # Convert to 28x28 images

# Plot 5 random images
fig, axes = plt.subplots(1, 5, figsize=(10, 5))
for i, ax in enumerate(axes):
    idx = np.random.randint(0, len(y_train))  # Random index
    ax.imshow(x_train_reshaped[idx], cmap="gray")
    ax.set_title(class_labels[y_train[idx]])
    ax.axis("off")
plt.show()

"""**Data preparation and validation**"""

# Separate labels and pixel
x_train = train_df.iloc[:, 1:].values
y_train = train_df.iloc[:, 0].values
x_test = test_df.iloc[:, 1:].values
y_test = test_df.iloc[:, 0].values

# Normalize pixel values to range [0,1]
x_train = x_train.astype("float32") / 255.0
x_test = x_test.astype("float32") / 255.0

# Reshape for model training
x_train = x_train.reshape(-1, 28, 28)
x_test = x_test.reshape(-1, 28, 28)

# Error handling validation
def validate_data(x_train, y_train, x_test, y_test):
    try:
        assert x_train is not None and y_train is not None, "Training data is missing."
        assert x_test is not None and y_test is not None, "Test data is missing."
        assert x_train.shape[0] == y_train.shape[0], "Mismatch in training samples and labels."
        assert x_test.shape[0] == y_test.shape[0], "Mismatch in test samples and labels."
        assert x_train.ndim == 3, f"Expected 3D input (samples, 28, 28), but got {x_train.shape}."
        assert x_test.ndim == 3, f"Expected 3D input (samples, 28, 28), but got {x_test.shape}."
        print("Data validation passed.")
    except AssertionError as e:
        print(f" Data validation error: {e}")
        raise

validate_data(x_train, y_train, x_test, y_test)

"""**Building the neural network and its different optimization/regularization**"""

# Neural network Sigmoid
model = Sequential()
model.add(Flatten(input_shape=(28, 28)))
model.add(Dense(128, activation="sigmoid"))
model.add(Dense(64, activation="sigmoid"))
model.add(Dense(10, activation="softmax"))

model.compile(optimizer=SGD(learning_rate=0.01),
              loss=SparseCategoricalCrossentropy(),
              metrics=['accuracy'])

model.summary()

history = model.fit(x_train, y_train,
                    epochs=10,
                    batch_size=1000,
                    validation_data=(x_test, y_test))

# Plot loss and accuracy curves
plt.figure(figsize=(12, 5))

plt.subplot(1, 2, 1)
plt.plot(history.history['loss'], label='Train Loss')
plt.plot(history.history['val_loss'], label='Test Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.title('Loss Curve')
plt.legend()

plt.subplot(1, 2, 2)
plt.plot(history.history['accuracy'], label='Train Accuracy')
plt.plot(history.history['val_accuracy'], label='Test Accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.title('Accuracy Curve')
plt.legend()

plt.show()

# ReLU activation
model_relu = Sequential()
model_relu.add(Flatten(input_shape=(28, 28)))
model_relu.add(Dense(128, activation="relu"))
model_relu.add(Dense(64, activation="relu"))
model_relu.add(Dense(10, activation="softmax"))

model_relu.compile(optimizer=SGD(learning_rate=0.01),
                   loss=SparseCategoricalCrossentropy(),
                   metrics=['accuracy'])

model_relu.summary()

history_relu = model_relu.fit(x_train, y_train,
                    epochs=10,
                    batch_size=1000,
                    validation_data=(x_test, y_test))

# Plot accuracy comparison
plt.figure(figsize=(12, 5))

plt.subplot(1, 2, 1)
plt.plot(history.history['accuracy'], label='Sigmoid - Train')
plt.plot(history.history['val_accuracy'], label='Sigmoid - Test')
plt.plot(history_relu.history['accuracy'], label='ReLU - Train')
plt.plot(history_relu.history['val_accuracy'], label='ReLU - Test')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.title('Accuracy Comparison: Sigmoid vs ReLU')
plt.legend()

# Plot loss comparison
plt.subplot(1, 2, 2)
plt.plot(history.history['loss'], label='Sigmoid - Train')
plt.plot(history.history['val_loss'], label='Sigmoid - Test')
plt.plot(history_relu.history['loss'], label='ReLU - Train')
plt.plot(history_relu.history['val_loss'], label='ReLU - Test')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.title('Loss Comparison: Sigmoid vs ReLU')
plt.legend()

plt.show()

# Dropout model
model_dropout = Sequential()
model_dropout.add(Flatten(input_shape=(28, 28)))
model_dropout.add(Dense(128, activation="relu"))
model_dropout.add(Dropout(0.2))  # 20% of neurons randomly turned off
model_dropout.add(Dense(64, activation="relu"))
model_dropout.add(Dropout(0.2))  # Another 20% dropout
model_dropout.add(Dense(10, activation="softmax"))


model_dropout.compile(optimizer=SGD(learning_rate=0.01),
                       loss=SparseCategoricalCrossentropy(),
                       metrics=['accuracy'])

model_dropout.summary()


history_dropout = model_dropout.fit(x_train, y_train,
                                    epochs=10,
                                    batch_size=1000,
                                    validation_data=(x_test, y_test))

# Plot Accuracy Comparison
plt.figure(figsize=(12, 5))

plt.subplot(1, 2, 1)
plt.plot(history.history['accuracy'], label='Sigmoid - Train')
plt.plot(history.history['val_accuracy'], label='Sigmoid - Test')
plt.plot(history_relu.history['accuracy'], label='ReLU - Train')
plt.plot(history_relu.history['val_accuracy'], label='ReLU - Test')
plt.plot(history_dropout.history['accuracy'], label='ReLU + Dropout - Train')
plt.plot(history_dropout.history['val_accuracy'], label='ReLU + Dropout - Test')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.title('Accuracy Comparison: Sigmoid vs ReLU vs ReLU + Dropout')
plt.legend()

# Plot Loss Comparison
plt.subplot(1, 2, 2)
plt.plot(history.history['loss'], label='Sigmoid - Train')
plt.plot(history.history['val_loss'], label='Sigmoid - Test')
plt.plot(history_relu.history['loss'], label='ReLU - Train')
plt.plot(history_relu.history['val_loss'], label='ReLU - Test')
plt.plot(history_dropout.history['loss'], label='ReLU + Dropout - Train')
plt.plot(history_dropout.history['val_loss'], label='ReLU + Dropout - Test')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.title('Loss Comparison: Sigmoid vs ReLU vs ReLU + Dropout')
plt.legend()

plt.show()

# ReLU Model 30 epochs
model_relu_30 = Sequential()
model_relu_30.add(Flatten(input_shape=(28, 28)))
model_relu_30.add(Dense(128, activation="relu"))
model_relu_30.add(Dense(64, activation="relu"))
model_relu_30.add(Dense(10, activation="softmax"))


model_relu_30.compile(optimizer=SGD(learning_rate=0.01),
                      loss=SparseCategoricalCrossentropy(),
                      metrics=['accuracy'])


history_relu_30 = model_relu_30.fit(x_train, y_train,
                                    epochs=30,
                                    batch_size=1000,
                                    validation_data=(x_test, y_test))

# ReLU + Dropout Model 30 epochs
model_dropout_30 = Sequential()
model_dropout_30.add(Flatten(input_shape=(28, 28)))
model_dropout_30.add(Dense(128, activation="relu"))
model_dropout_30.add(Dropout(0.2))
model_dropout_30.add(Dense(64, activation="relu"))
model_dropout_30.add(Dropout(0.2))
model_dropout_30.add(Dense(10, activation="softmax"))


model_dropout_30.compile(optimizer=SGD(learning_rate=0.01),
                         loss=SparseCategoricalCrossentropy(),
                         metrics=['accuracy'])


history_dropout_30 = model_dropout_30.fit(x_train, y_train,
                                          epochs=30,
                                          batch_size=1000,
                                          validation_data=(x_test, y_test))

# Plot accuracy comparison
plt.figure(figsize=(12, 5))

plt.subplot(1, 2, 1)
plt.plot(history_relu_30.history['accuracy'], label='ReLU - Train')
plt.plot(history_relu_30.history['val_accuracy'], label='ReLU - Test')
plt.plot(history_dropout_30.history['accuracy'], label='ReLU + Dropout - Train')
plt.plot(history_dropout_30.history['val_accuracy'], label='ReLU + Dropout - Test')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.title('Accuracy Comparison: ReLU vs ReLU + Dropout (30 Epochs)')
plt.legend()

# Plot loss comparison
plt.subplot(1, 2, 2)
plt.plot(history_relu_30.history['loss'], label='ReLU - Train')
plt.plot(history_relu_30.history['val_loss'], label='ReLU - Test')
plt.plot(history_dropout_30.history['loss'], label='ReLU + Dropout - Train')
plt.plot(history_dropout_30.history['val_loss'], label='ReLU + Dropout - Test')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.title('Loss Comparison: ReLU vs ReLU + Dropout (30 Epochs)')
plt.legend()

plt.show()